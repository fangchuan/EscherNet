{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "sys.path.append('./')\n",
    "sys.path.append('../')\n",
    "sys.path.insert(0, '../6DoF/')\n",
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "import trimesh\n",
    "import logging\n",
    "from logging import getLogger as get_logger\n",
    "\n",
    "from koolai_dataset import KoolAIPanoData, KoolAIPersData\n",
    "from utils.typing import *\n",
    "from utils.misc import get_device, todevice\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformation matrix for visualization in opengl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_gl_cv = np.asarray([\n",
    "            [1.0,  0.0,  0.0],\n",
    "            [0.0, -1.0,  0.0],\n",
    "            [0.0,  0.0, -1.0],\n",
    "        ])\n",
    "R_cv_gl = np.linalg.inv(R_gl_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Train a model on KoolAID dataset')\n",
    "    parser.add_argument('--root_data_dir', type=str, default='/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/')\n",
    "    parser.add_argument('--train_split_file', type=str, default='/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/train.txt')\n",
    "    parser.add_argument('--test_split_file', type=str, default='/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/test.txt')\n",
    "    parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "    parser.add_argument('--dataloader_num_workers', type=int, default=4)\n",
    "    parser.add_argument('--skip_calc_scene_scale', action='store_true')\n",
    "    parser.add_argument('--skip_save_scene_scale', action='store_true')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def merge_split_files(file_path_lst: List[str], merge_file_path: str):\n",
    "    with open(merge_file_path, 'w') as f:\n",
    "        for file_path in file_path_lst:\n",
    "            with open(file_path, 'r') as f_split:\n",
    "                for line in f_split.readlines():\n",
    "                    f.write(line)\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= length of dataset 202 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.num_all_rooms: 202, self.num_all_views: 81272\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/3FO4K5FWGG13/perspective/room_702/depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset)):\n\u001b[1;32m     63\u001b[0m     room_uid \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mroom_ids[idx]\n\u001b[0;32m---> 64\u001b[0m     room_distance_scale \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_room_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     room_scale_lst\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor([room_distance_scale], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     66\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m room \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroom_uid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m distance scale: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroom_distance_scale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/seaweedfs/training/experiments/zhenqing/EscherNet/scripts/../6DoF/koolai_dataset.py:650\u001b[0m, in \u001b[0;36mcompute_room_scale\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    647\u001b[0m target_images: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN 3 H W\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m rgbs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT_in:]\n\u001b[1;32m    649\u001b[0m all_depths: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN 1 H W\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m depths\n\u001b[0;32m--> 650\u001b[0m input_depths: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN 1 H W\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_depths[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT_in]\n\u001b[1;32m    651\u001b[0m target_depths: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN 1 H W\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_depths[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT_in:]\n\u001b[1;32m    653\u001b[0m cond_Ts: Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN 4 4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poses[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT_in]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/3FO4K5FWGG13/perspective/room_702/depth'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':    \n",
    "    \n",
    "    root_data_dir = '/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/'\n",
    "    train_split_file = '/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/perspective_train.txt'\n",
    "    test_split_file = '/seaweedfs/training/dataset/qunhe/PanoRoom/processed_data_20240312/perspective_test.txt'\n",
    "    train_batch_size = 8\n",
    "    dataloader_num_workers = 4\n",
    "    skip_calc_scene_scale = False\n",
    "    skip_save_scene_scale = False\n",
    "    \n",
    "    device = get_device()\n",
    "    \n",
    "    # in case the data was processed by kfp, it will results in many train.txt-0000xx files and test.txt-0000xx files\n",
    "    train_file_paths = glob.glob(train_split_file + '*')\n",
    "    test_file_paths = glob.glob(test_split_file + '*')\n",
    "    \n",
    "    assert len(train_file_paths) >= 1, f'found train split files: {train_file_paths}'\n",
    "    if len(train_file_paths) > 1 and len(test_file_paths) > 1:\n",
    "        if 'paorama_train' in train_split_file:\n",
    "            save_train_split_file = train_split_file.replace('paorama_train.txt', 'panorama_train.txt')\n",
    "        else:\n",
    "            save_train_split_file = train_split_file\n",
    "        save_test_split_file = test_split_file\n",
    "        # merge split files into a single one\n",
    "        merge_split_files(file_path_lst=train_file_paths, merge_file_path=save_train_split_file)\n",
    "        merge_split_files(file_path_lst=test_file_paths, merge_file_path=save_test_split_file)\n",
    "        # remove the original split files\n",
    "        os.system(f'rm {train_split_file}-0000*')\n",
    "        os.system(f'rm {test_split_file}-0000*')\n",
    "        \n",
    "        train_split_file = save_train_split_file\n",
    "        test_split_file = save_test_split_file\n",
    "    else:\n",
    "        train_split_file = train_file_paths[0]\n",
    "        test_split_file = test_file_paths[0]\n",
    "    \n",
    "    # prepare dataset\n",
    "    T_in = 8\n",
    "    T_out = 8\n",
    "    train_dataset = KoolAIPersData(root_dir=root_data_dir,\n",
    "                                   split_filepath=train_split_file,\n",
    "                                    image_height=256, \n",
    "                                    image_width=256,\n",
    "                                    total_view=16,\n",
    "                                    validation=False,\n",
    "                                    T_in=T_in,\n",
    "                                    T_out=T_out,\n",
    "                                    fix_sample=False,)\n",
    "    \n",
    "    # for training\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=train_batch_size,\n",
    "        num_workers=dataloader_num_workers,\n",
    "        collate_fn=train_dataset.collate\n",
    "    )\n",
    "    \n",
    "    if not skip_calc_scene_scale:\n",
    "    # calculate all rooms' scale\n",
    "        room_scale_lst = []\n",
    "        for idx in range(len(train_dataset)):\n",
    "            room_uid = train_dataset.room_ids[idx]\n",
    "            room_distance_scale = train_dataset.compute_room_scale(idx)\n",
    "            room_scale_lst.append(torch.tensor([room_distance_scale], dtype=torch.float32))\n",
    "            logger.info(f' room {room_uid} distance scale: {room_distance_scale}')\n",
    "\n",
    "            \n",
    "        room_scales = torch.cat(room_scale_lst, dim=0)    \n",
    "        # draw room scale histogram\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.hist(room_scales.cpu().numpy(), bins=100)\n",
    "        plt.show()\n",
    "        \n",
    "        # filter room_scales \n",
    "        import scipy.stats\n",
    "        zscore = scipy.stats.zscore(room_scales.cpu().numpy())\n",
    "        filtered_room_entries = np.abs(zscore) < 3\n",
    "        logger.info(f'max zscore: {np.max(zscore)}')\n",
    "        \n",
    "        # we take 95% quantile as the new scene scale, to remove some rooms with invaliid scale\n",
    "        # new_scene_scale = torch.quantile(room_scales, 0.99).item()    \n",
    "        new_scene_scale = torch.max(room_scales[filtered_room_entries]).item()\n",
    "        logger.info(f'training dataset room scale: {new_scene_scale}')\n",
    "    else:\n",
    "        # TODO: manually set the new_scene_scale\n",
    "        new_scene_scale = 7.609875202178955\n",
    "    new_scene_scale = 1.0 / new_scene_scale\n",
    "    \n",
    "    if not skip_save_scene_scale:\n",
    "        for idx in range(len(train_dataset)):\n",
    "            room_uid = train_dataset.room_ids[idx]\n",
    "            room_folderpath = os.path.join(root_data_dir, room_uid)\n",
    "            room_meta_filepath = os.path.join(room_folderpath, 'room_meta.json')\n",
    "            logger.info(f'correct scene scale for room: {room_meta_filepath}')\n",
    "            \n",
    "            with open(room_meta_filepath, 'r') as f:\n",
    "                room_meta = json.load(f)\n",
    "                \n",
    "            scale_mat = np.array(room_meta['scale_mat']).reshape(4,4).astype(np.float32)\n",
    "            original_scale = float(room_meta['scale'])\n",
    "            \n",
    "            # savee corrected scale_mat\n",
    "            camera_center = scale_mat[:3, 3] / original_scale\n",
    "            \n",
    "            new_scale_mat = np.eye(4).astype(np.float32)\n",
    "            new_scale_mat[:3, 3] = camera_center\n",
    "            new_scale_mat[:3] *= new_scene_scale\n",
    "            \n",
    "            room_meta['new_scale_mat'] = new_scale_mat.flatten().tolist()\n",
    "            room_meta['new_scale'] = new_scene_scale\n",
    "            json.dump(room_meta, open(room_meta_filepath, 'w'), indent=4)\n",
    "            \n",
    "            # visualize normalized camera poses\n",
    "            pose_mesh = o3d.geometry.TriangleMesh()\n",
    "            \n",
    "            camera_metas = room_meta['cameras']\n",
    "            for cam_idx in range(len(camera_metas)):     \n",
    "                cam_meta = camera_metas[str(cam_idx)]             \n",
    "                # w2c pose\n",
    "                pose = np.array(cam_meta['camera_transform']).reshape(4, 4)\n",
    "                c2w_pose = np.linalg.inv(pose)\n",
    "                # scale pose_c2w\n",
    "                c2w_pose = new_scale_mat @ c2w_pose\n",
    "                R_c2w = c2w_pose[:3, :3]\n",
    "                q_c2w = trimesh.transformations.quaternion_from_matrix(R_c2w)\n",
    "                q_c2w = trimesh.transformations.unit_vector(q_c2w)\n",
    "                R_c2w = trimesh.transformations.quaternion_matrix(q_c2w)[:3, :3]\n",
    "                c2w_pose[:3, :3] = R_c2w\n",
    "                \n",
    "                T = c2w_pose\n",
    "                T[:3, :3] = T[:3, :3] @ R_cv_gl\n",
    "                pose_mesh += o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.01).transform(T)\n",
    "                o3d.io.write_triangle_mesh(os.path.join(room_folderpath, 'pose_mesh.ply'), pose_mesh)\n",
    "            \n",
    "\n",
    "    # # test on a specific room\n",
    "    # room_folderpath = os.path.join(root_data_dir, '3FO4K5FWN1Q2/panorama/room_824')\n",
    "    # room_meta_filepath = os.path.join(room_folderpath, 'room_meta.json')\n",
    "    # logger.info(f'correct scene scale for room: {room_meta_filepath}')\n",
    "        \n",
    "    # with open(room_meta_filepath, 'r') as f:\n",
    "    #     room_meta = json.load(f)\n",
    "        \n",
    "    # scale_mat = np.array(room_meta['scale_mat']).reshape(4,4).astype(np.float32)\n",
    "    # original_scale = float(room_meta['scale'])\n",
    "    \n",
    "    # # savee corrected scale_mat\n",
    "    # camera_center = scale_mat[:3, 3] / original_scale\n",
    "    # logger.info(f'original camera_center: {-camera_center}')\n",
    "    \n",
    "    # new_scale_mat = np.eye(4).astype(np.float32)\n",
    "    # new_scale_mat[:3, 3] = camera_center\n",
    "    # new_scale_mat[:3] *= new_scene_scale\n",
    "    \n",
    "    # room_meta['new_scale_mat'] = new_scale_mat.flatten().tolist()\n",
    "    # room_meta['new_scale'] = new_scene_scale\n",
    "    # json.dump(room_meta, open(room_meta_filepath, 'w'), indent=4)\n",
    "    \n",
    "    # # visualize normalized camera poses\n",
    "    # pose_mesh = o3d.geometry.TriangleMesh()\n",
    "    \n",
    "    # camera_metas = room_meta['cameras']\n",
    "    # for cam_idx in range(len(camera_metas)):     \n",
    "    #     cam_meta = camera_metas[str(cam_idx)]       \n",
    "    #     # w2c pose\n",
    "    #     pose = np.array(cam_meta['camera_transform']).reshape(4, 4)\n",
    "    #     c2w_pose = np.linalg.inv(pose)\n",
    "    #     # scale pose_c2w\n",
    "    #     c2w_pose = new_scale_mat @ c2w_pose\n",
    "    #     R_c2w = c2w_pose[:3, :3]\n",
    "    #     q_c2w = trimesh.transformations.quaternion_from_matrix(R_c2w)\n",
    "    #     q_c2w = trimesh.transformations.unit_vector(q_c2w)\n",
    "    #     R_c2w = trimesh.transformations.quaternion_matrix(q_c2w)[:3, :3]\n",
    "    #     c2w_pose[:3, :3] = R_c2w\n",
    "        \n",
    "    #     T = c2w_pose\n",
    "    #     T[:3, :3] = T[:3, :3] @ R_cv_gl\n",
    "    #     pose_mesh += o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.01).transform(T)\n",
    "        \n",
    "    #     o3d.io.write_triangle_mesh(os.path.join(room_folderpath, 'pose_mesh.ply'), pose_mesh)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
